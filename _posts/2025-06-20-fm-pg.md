---
layout: distill
title: Flow Matching Policy Gradients
description: "Simple Online Reinforcement Learning with Flow Matching"
tags: distill formatting
giscus_comments: false
date: 2025-06-20
permalink: /
featured: true

# Add this line to set a custom accent color
theme_color: "#EEEEEE"   # You can use any hex color code here

mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true
code_highlighting: true

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Flow Matching
  - name: On-Policy RL - Sample, Score, Reinforce
  - name: Flow Matching Policy Gradients
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .highlight-python {
    color: #18327E;
  }
  .highlight-comment {
    color: #a31515;
  }
---

<div style="text-align: center; margin-bottom: 20px;">
  <a href="https://mcallisterdavid.com/" style="text-decoration: none; margin: 18px 18px; font-weight: bold;">
    David McAllister*
  </a>
  <a href="https://songweige.github.io" style="text-decoration: none; margin: 18px 18px; font-weight: bold;">
    Songwei Ge*
  </a>
  <a href="https://brentyi.github.io/" style="text-decoration: none; margin: 18px 18px; font-weight: bold;">
    Brent Yi*
  </a>
  <a href="https://chungmin99.github.io" style="text-decoration: none; margin: 0 10px; font-weight: bold;">
    Chung Min Kim
  </a>
  <br>
  <a href="https://ethanweber.me" style="text-decoration: none; margin: 0 10px; font-weight: bold;">
    Ethan Weber
  </a>
  <a href="https://hongsukchoi.github.io/" style="text-decoration: none; margin: 18px 18px; font-weight: bold;">
    Hongsuk Choi
  </a>
  <a href="https://havenfeng.github.io" style="text-decoration: none; margin: 0 10px; font-weight: bold;">
    Haiwen Feng
  </a>
  <a href="https://people.eecs.berkeley.edu/~kanazawa/" style="text-decoration: none; margin: 0 10px; font-weight: bold;">
    Angjoo Kanazawa
  </a>
</div>
<div>
{% include video.liquid path="assets/video/fpo_blog_teaser_v2.mp4" class="img-fluid rounded" controls=false autoplay=true loop=true muted=true width="100%" height="100%" %}
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <a href="https://arxiv.org/pdf/2501.05450" class="btn btn-lg z-depth-0" role="button" style="text-decoration: none; border: 1px solid #ccc; margin: 0 5px; padding: 10px 20px;">
    <i class="fas fa-file-pdf"></i> Paper
  </a>
  <a href="https://arxiv.org/abs/2501.05450" class="btn btn-lg z-depth-0" role="button" style="text-decoration: none; border: 1px solid #ccc; margin: 0 5px; padding: 10px 20px;">
    <i class="ai ai-arxiv"></i> arXiv
  </a>
</div>

<!-- ## Motivation -->

Flow models have become the go-to approach to model distributions in continuous space. They soak up data with a simple, scalable denoising objective and now represent the state-of-the art in generating images, videos, audio and, more recently, robot action. However, they are still unpopular for learning from rewards through reinforcement learning.

Meanwhile, to perform RL in continuous spaces, practicioners typically train far simpler Gaussian policies, which represent a single, ellipsoidal mode of the action distribution. Flow-based policies can capture complex, multimodal action distributions, but they are primarily trained in a supervised manner with behavior cloning (BC). We show that it's possible to train RL policies using flow matching, the framework behind modern diffusion and flow models, to benefit from its expressivity.

We approached this project as researchers primarily familiar with diffusion models. While working on <a href="https://videomimic.net">VideoMimic</a>, we felt limited by the expressiveness of Gaussian policies and thought diffusion could help. In this blog post, we'll explain how we connect flow matching and on-policy RL in a way that makes sense without an extensive RL background.

We introduce <b>Flow Policy Optimization</b> (FPO), a new algorithm to train RL policies with flow matching. It can train expressive flow policies from only rewards. We find its particularly useful to learn underconditioned policies, like humanoid locomotion with simple joystick commands.

## Flow Matching

Flow matching optimizes a model to transform a simple distribution (e.g., the Gaussian distribution) into a complex one through a multi-step mapping called the marginal flow. We expand on the marginal flow in more detail in another blog post for <a href="https://decentralizeddiffusion.github.io">Decentralized Diffusion Models</a>. The flow smoothly directs a particle $x_t$ to the data distribution, so integrating the flow across time leads to a data sample. We can actually calculate the marginal flow *analytically*, which we do in real-time in the plot below:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/flow_sde_plot.html' | relative_url }}" frameborder='0' scrolling='no' width="120%" style="margin-left: -10%; height: auto; min-height: 510px;"></iframe>
</div>

Each particle here represent an $x_t$ noisy latent that gets iteratively denoised as the time goes from zero to one. If you're on desktop, drag the control points of the modes on the right to see how the underlying PDF and the particle trajectories change. Notice how the probability mass flows smoothly from the initial noise to form two distinct modes. The multi-step mapping is the magic that lets flow models transform a simple, tractable distribution into one of arbitrary complexity.

While it's simple to compute this flow in 1D, it becomes intractable over large datasets in high dimensional space. Instead, we use flow matching, which compresses the marginal flow into a neural network through a simple reconstruction objective.

<div class="l-body" style="text-align: center; margin-top: -4%; margin-bottom: -4%;">
  <img src="{{ '/assets/img/fpo/flow_matching.svg' | relative_url }}" alt="DDM Overview" style="margin-left: -5%; width: 110%; height: auto; clip-path: inset(0px 0 0px 0);">
</div>
<div class="caption" style="margin-top: 0px; margin-bottom: 2%;">
    Flow matching the conditional flow $u_t(x_t|x)$ and velocity prediction $v_t(x_t)$.
</div>

Flow matching perturbs a clean data sample with Gaussian noise then tasks the model with reconstructing the sample by predicting the velocity, which is the derivative of $x_t$'s position *w.r.t.* time. In expectation over a fixed dataset, this optimization recovers the marginal flow for any $x_t$. Integrating $x_t$'s position across time along the marginal flow will recover a sample from the data distribution.

Geometrically, the marginal flow points to a *weighted-average* of the data where the weights are a function of the timestep and distance from $x_t$ to each data point. You can see the particles follow the marginal flow exactly in the plot above when stochasticity is turned off. Stated simply, flow matching learns to point the model's flow field, $v_t(x_t)$, to the data distribution.

Flow matching has statistical significance too. Instead of computing exact likelihoods (expensive and unstable), it optimizes a lower bound called the Evidence Lower Bound (ELBO). This pushes the model toward higher likelihoods without computing them directly. In the limit, the flow model will sample exactly from the probability distribution of the dataset. So if you've learned the flow function well, you've learned the underlying structure of the data.

<b>Flowing toward a data point increases its likelihood under the model.</b>

## On-Policy RL: Sample, Score, Reinforce

On-policy reinforcement learning follows a basic core loop: sample from your policy, score each action with rewards, then make high-reward actions more likely. Rinse and repeat.

This procedure climbs the policy gradient---the gradient of expected cumulative reward. Your model collects "experience" from by sampling its learned distribution, sees which samples are most advantageous, and adjusts to perform similar actions more often.

On-policy RL can be cast as search iteratively distilled into a model. The policy "happens upon" good behaviors through exploration, then reinforces them. Over time, it discovers the patterns in the random successes and develops reliable strategies. You can start from a pretrained model and continue training with RL to explore within a rich prior rather than at random. This is the dominant approach to upcycle LLMs for preference alignment and reasoning.

### Image Generation Analogy

Here's an example RL loop to understand (replace with DMControl rollout videos from Brent):

- Generate a batch of images from your model (rollouts)
- Score each image with a reward (maybe "how much does this look like a dog?")
- Train your model to boost the likelihood of high-scoring images
- Repeat until your model reliably generates high-reward images

<b>Sample and score images:</b>

<div class="l-body" style="text-align: center; margin-top: -0%; margin-bottom: 4%;">
  <img src="{{ '/assets/img/fpo/dog_rewards.png' | relative_url }}" alt="DDM Overview" style="margin-left: -1%; width: 102%; height: auto; clip-path: inset(0px 0 0px 0);">
</div>

From the rewards, we calculate advantages. These can be viewed as the reward normalized *w.r.t.* the expected reward from the rest of the rollout under the current policy. This expected reward is what you learn with a critic in PPO<d-cite key="schulman2017proximalpolicyoptimizationalgorithms"></d-cite> or compute as the average of a group's reward in GRPO<d-cite key="shao2024deepseekmathpushinglimitsmathematical"></d-cite>.

<b>Calculate each advantage and form policy gradient:</b>

<div class="l-body" style="text-align: center; margin-top: -0%; margin-bottom: 2%;">
  <img src="{{ '/assets/img/fpo/dog_adv.png' | relative_url }}" alt="DDM Overview" style="margin-left: -1%; width: 102%; height: auto; clip-path: inset(0px 0 0px 0);">
</div>

 <!-- using popular policy gradient methods. -->

 Given the advantages, train the model on each data point with a gradient update scaled by the corresponding advantage. So, if the advantage is negative, it will become less likely. Postive advantage, more likely.


## Flow Matching Policy Gradients

To reiterate, the goal of on-policy RL is simple: increase the likelihood of high-reward actions. Meanwhile, flow matching naturally increases likelihoods by redirecting probability flow toward training samples. This makes our objective clear---<b>redirect the flow toward high reward actions</b>.

In the limit of perfect optimization, flow matching assigns probabilities according to the frequency of samples in your training set. Since we're using RL, that "training set" is dynamically generated from the model.

Advantages make the connection between synthetic data generation and on-policy RL explicit. In RL, we calculate the advantage of each sampled action, a quantity that indicates how much better it was than expected. These advantages are centered around zero to reduce variance: positive for better-than-expected actions, negative for worse. Advantages then become a *loss weighting* in the policy gradient. As a simple example, if an action is very advantageous, the model encounters a scaled-up loss on it and learns to boost it aggressively.

<div class="l-body" style="text-align: center;">
  <img src="{{ '/assets/img/fpo/policy_grad.svg' | relative_url }}" alt="DDM Overview" style="width: 100%; height: auto; margin-top: 2%;">
</div>
<div class="caption">
    The policy gradient resembles a standard supervised learning gradient on the model's own synthetic samples with the loss scaled by the reward or advantage (both are valid).
</div>

Zero-mean advantages are fine for RL in discrete spaces because a negative advantage simply pushes down the logit of a suboptimal action, and the softmax ensures that the resulting action probabilities remain valid and non-negative. Flow matching, however, learns probability flows to sample from a training data distribution. These must be nonnegative by construction, so negative loss weights break the interpretation.

There's a simple solution: make the advantages nonnegative. Shifting advantages by a constant doesn't change the policy gradient. In fact, this is the mathematical property that lets us use advantages instead of raw rewards in the first place. Here's how we can understand non-negative advantages in the flow matching framework:

<div class="l-body" style="text-align: center;">
  <img src="{{ '/assets/img/fpo/marginal_flow_fpo.svg' | relative_url }}" alt="DDM Overview" style="width: 94%; height: auto; margin-top: 2%;">
</div>

<div class="caption">
    The marginal flow is a linear combination of the (conditional) flow to each data point. The weighting of each path scales with probability of drawing the data point from the dataset, $q(x)$.
</div>

Advantages manifest as loss-weighting, which can be intuitively expressed in the marginal flow framework. The marginal flow is the weighted average of the paths (the $u_t$'s) from the current noisy particle, $x_t$, to each data point $x$. The paths are also weighed by $q(x)$, the probability of drawing $x$ from your training set. This is typically a constant $\frac{1}{N}$ for a dataset of size $N$, assuming every data point is unique. Loss weights are equivalent to altering the frequency of the data points in your training set. If the loss for a data point is scaled by a factor of 2, its equivalent to that data point showing up twice in the train set.

Now, we can get a complete picture of the algorithm that connects flow matching and reinforcement learning:

- Generate actions from your flow model using your choice of sampler
- Score them with rewards and compute advantages
- Flow match on the advantage-weighed actions

This procedure boosts the likelihood of actions that achieve high reward while preserving the desirable properties of flow models---multimodality, expressivity and the improved exploration that stems from them. We visualize the procedure below in the same 1D flow diagram as above:


<div class="l-page">
  <iframe src="{{ '/assets/plotly/advantage_flow_plot.html' | relative_url }}" frameborder='0' scrolling='no' width="120%" style="margin-left: -10%; height: auto; min-height: 510px;"></iframe>
</div>

## Acknowledgements

We thank Qiyang (Colin) Li, Oleg Rybkin, Lily Goli and Michael Psenka for helpful discussions and feedback on the manuscript. We thank Arthur Allshire, Tero Karras, Miika Aittala, Kevin Zakka and Seohong Park for insightful input and feedback on implementation details and the broader context of this work.

